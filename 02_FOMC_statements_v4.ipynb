{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finm35000 Data project 1\n",
    "\n",
    "Team Members:\n",
    "- Antonio Pineda Acosta\n",
    "- Kaleem Shah Bukhari\n",
    "- Raafay Uqailya\n",
    "- Yasmine Ouattara"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Measuring the tone of FOMC statements (90 points)\n",
    "\n",
    "#### You will follow the method in Tadle (2022). The paper is linked on Canvas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import string\n",
    "from fredapi import Fred\n",
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import datetime as dt\n",
    "import statsmodels.api as sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (10 points) Scrape the text of the FOMC statements from January 2000 to the present. \n",
    "\n",
    "Use https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm for 2019-2024 and https://www.federalreserve.gov/monetarypolicy/fomc historical year.htm for 2000-2018.\n",
    "\n",
    "Here are some hints provided by Joanna for scraping if scraping is new to you.\n",
    "\n",
    "- Use the BeautifulSoup package to extract the html from the website. Load this package by running the code: from bs4 import BeautifulSoup.\n",
    "\n",
    "- Use the command soup.findall(‘a’,href=True) to get a list of all the URL links on the web page.\n",
    "\n",
    "- Inspect the URLs to figure out what the format of the links to statement files look like and narrow your list of links to only include these.\n",
    "\n",
    "- Iterate through the links to the statements and again use the BeautifulSoup package to extract the text.\n",
    "\n",
    "- Inspect the text to remove extraneous parts from the start and end. Make sure to remove HTML tags.\n",
    "\n",
    "- You will need the date of the statement to merge the text to the returns. It can be found in the URL. For example, https: //www.federalreserve.gov/newsevents/pressreleases/monetary20220126a.htm  was released on Jan. 26, 2022, which is at the end of the URL.\n",
    "\n",
    "Report the following after you extract the text: 1) Number of statements you obtain and the list of the dates of their release. The statements are released shortly after each FOMC meeting. 2) A summary statistic of the number of words in statements including mean, standard deviation, minimum, first quartile, median, third quartile, maximum."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2019-2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Statement Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2024-01-31</td>\n",
       "      <td>January 31, 2024\\nFederal Reserve issues FOMC ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-03-20</td>\n",
       "      <td>March 20, 2024\\nFederal Reserve issues FOMC st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>May 01, 2024\\nFederal Reserve issues FOMC stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-06-12</td>\n",
       "      <td>June 12, 2024\\nFederal Reserve issues FOMC sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-07-31</td>\n",
       "      <td>July 31, 2024\\nFederal Reserve issues FOMC sta...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                     Statement Text\n",
       "0  2024-01-31  January 31, 2024\\nFederal Reserve issues FOMC ...\n",
       "1  2024-03-20  March 20, 2024\\nFederal Reserve issues FOMC st...\n",
       "2  2024-05-01  May 01, 2024\\nFederal Reserve issues FOMC stat...\n",
       "3  2024-06-12  June 12, 2024\\nFederal Reserve issues FOMC sta...\n",
       "4  2024-07-31  July 31, 2024\\nFederal Reserve issues FOMC sta..."
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to extract the statement links from the 2019-2024 URL\n",
    "def extract_statement_links_2019_2024(url):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        url (str): The URL of the webpage to scrape.\n",
    "    Output:\n",
    "        list of str: A list of URLs containing FOMC press release statements.\n",
    "    \"\"\"\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    \n",
    "    # Extract all URLs that contain 'pressrelease' (which likely include FOMC statements)\n",
    "    fomc_links = soup.find_all('a', href=True)\n",
    "    \n",
    "    # Filter links for FOMC statements based on a common pattern in the URL\n",
    "    statement_links = [link['href'] for link in fomc_links if 'pressrelease' in link['href']]\n",
    "    return statement_links\n",
    "\n",
    "# Function to clean and extract the relevant FOMC statement text\n",
    "def extract_statement_text_2019_2024(links):\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        links (list): A list of FOMC statements links\n",
    "    Ouput:\n",
    "        statements (list), dates (list): A list of relevant statements & A list of release dates\n",
    "    \"\"\"\n",
    "    base_url = \"https://www.federalreserve.gov\"\n",
    "    statements = []\n",
    "    dates = []\n",
    "    \n",
    "    for link in links:\n",
    "        full_url = base_url + link  # Construct the full URL\n",
    "        statement_response = requests.get(full_url)\n",
    "        \n",
    "        # Fix encoding issues by decoding the content as UTF-8\n",
    "        statement_soup = BeautifulSoup(statement_response.content.decode('utf-8'), 'html.parser')\n",
    "        \n",
    "        # Extract the entire text of the page\n",
    "        full_text = statement_soup.get_text().strip()\n",
    "\n",
    "        # Find the start of the relevant FOMC statement\n",
    "        start_marker = \"Please enable JavaScript if it is disabled in your browser or access the information through the links provided below.\"\n",
    "        end_marker = \"About the FedNews & EventsMonetary PolicySupervision & RegulationFinancial StabilityPayment SystemsEconomic ResearchDataConsumers & CommunitiesConnect with the Board\"\n",
    "        \n",
    "        start_index = full_text.find(start_marker)\n",
    "        end_index = full_text.find(end_marker)\n",
    "        \n",
    "        if start_index != -1 and end_index != -1:\n",
    "            # Extract the relevant portion of the text between the start and end markers\n",
    "            relevant_text = full_text[start_index + len(start_marker):end_index]\n",
    "            \n",
    "            # Extract date from the URL using regex (as an example of 20220126a.htm)\n",
    "            match = re.search(r'(\\d{8})a\\.htm', link)\n",
    "            \n",
    "            if relevant_text and match:  # Ensure both statement text and date are found\n",
    "                date = match.group(1)  # Extracts the date part (e.g., 20220126)\n",
    "                dates.append(date)\n",
    "                statements.append(relevant_text.strip())\n",
    "    \n",
    "    return statements, dates\n",
    "\n",
    "# URL for scraping 2019-2024 statements\n",
    "url_2019_2024 = \"https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm\"\n",
    "\n",
    "# Extract statement links for the 2019-2024 period\n",
    "statement_links_2019_2024 = extract_statement_links_2019_2024(url_2019_2024)\n",
    "\n",
    "# Extract statement texts and dates for the 2019-2024 period\n",
    "statements_2019_2024, dates_2019_2024 = extract_statement_text_2019_2024(statement_links_2019_2024)\n",
    "\n",
    "# Create a dataframe for the 2019-2024 period\n",
    "data_2019_2024 = {'Date': dates_2019_2024, 'Statement Text': statements_2019_2024}\n",
    "df_2019_2024 = pd.DataFrame(data_2019_2024)\n",
    "\n",
    "#from Date make this format 2000-01-19\n",
    "df_2019_2024['Date'] = pd.to_datetime(df_2019_2024['Date'], format='%Y%m%d')\n",
    "df_2019_2024['Date'] = df_2019_2024['Date'].dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Save the DataFrame to an Excel file\n",
    "output_file_path_2019_2024 = 'Data/FOMC_statements_2019_2024.xlsx'\n",
    "df_2019_2024.to_excel(output_file_path_2019_2024, index = False)\n",
    "\n",
    "# Display the DataFrame for verification\n",
    "df_2019_2024.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2000-2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of specific dates to remove\n",
    "dates_to_remove = [\n",
    "    \"1996-10-01\", \"1996-12-01\", \"1997-01-01\", \"1997-03-01\", \"1997-05-01\", \"1997-06-01\",\n",
    "    \"1997-08-01\", \"1997-09-01\", \"1997-10-01\", \"1997-12-01\", \"1998-01-01\", \"1998-03-01\",\n",
    "    \"1998-05-01\", \"1998-06-01\", \"1998-08-01\", \"1998-09-01\", \"1998-11-01\", \"1998-12-01\",\n",
    "    \"1999-01-01\", \"1999-03-01\", \"1999-05-01\", \"1999-06-01\", \"1999-08-01\", \"1999-09-01\",\n",
    "    \"1999-11-01\", \"1999-12-01\", \"2000-01-01\", \"2000-03-01\", \"2000-05-01\", \"2000-06-01\",\n",
    "    \"2000-08-01\", \"2000-09-01\", \"2000-11-01\", \"2000-12-01\", \"2001-01-01\", \"2001-03-01\",\n",
    "    \"2001-05-01\", \"2001-06-01\", \"2001-08-01\", \"2001-09-01\", \"2001-10-01\", \"2001-11-01\",\n",
    "    \"2002-01-01\", \"2002-03-01\", \"2002-04-01\", \"2002-06-01\", \"2002-07-01\", \"2002-10-01\",\n",
    "    \"2002-11-01\", \"2003-01-01\", \"2003-03-01\", \"2003-04-01\", \"2003-06-01\", \"2003-07-01\",\n",
    "    \"2003-10-01\", \"2003-11-01\", \"2004-01-01\", \"2004-03-01\", \"2004-04-01\", \"2004-06-01\",\n",
    "    \"2004-07-01\", \"2004-09-01\", \"2004-10-01\", \"2004-12-01\", \"2005-01-01\", \"2005-03-01\",\n",
    "    \"2005-04-01\", \"2005-06-01\", \"2005-07-01\", \"2005-09-01\", \"2005-10-01\", \"2005-11-01\",\n",
    "    \"2006-01-01\", \"2006-03-01\", \"2006-04-01\", \"2006-06-01\", \"2006-07-01\", \"2006-09-01\",\n",
    "    \"2006-10-01\", \"2006-11-01\", \"2007-01-01\", \"2007-03-01\", \"2007-04-01\", \"2007-06-01\",\n",
    "    \"2007-07-01\", \"2007-09-01\", \"2007-10-01\", \"2007-11-01\", \"2008-01-01\", \"2008-03-01\",\n",
    "    \"2008-04-01\", \"2008-06-01\", \"2008-07-01\", \"2008-09-01\", \"2008-10-01\", \"2008-12-01\",\n",
    "    \"2009-01-01\", \"2009-03-01\", \"2009-04-01\", \"2009-06-01\", \"2009-07-01\", \"2009-09-01\",\n",
    "    \"2009-10-01\", \"2009-12-01\", \"2010-01-01\", \"2010-03-01\", \"2010-04-01\", \"2010-06-01\",\n",
    "    \"2010-07-01\", \"2010-09-01\", \"2010-10-01\", \"2010-12-01\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing year: 2000\n",
      "Processing year: 2001\n",
      "Processing year: 2002\n",
      "Processing year: 2003\n",
      "Processing year: 2004\n",
      "Processing year: 2005\n",
      "Processing year: 2006\n",
      "Processing year: 2007\n",
      "Processing year: 2008\n",
      "Processing year: 2009\n",
      "Processing year: 2010\n",
      "Processing year: 2011\n",
      "Processing year: 2012\n",
      "Processing year: 2013\n",
      "Processing year: 2014\n",
      "Processing year: 2015\n",
      "Processing year: 2016\n",
      "Processing year: 2017\n",
      "Processing year: 2018\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Statement Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-19</td>\n",
       "      <td>Reports from most Federal Reserve Districts in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-03-08</td>\n",
       "      <td>Reports from the twelve Federal Reserve Distri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-05-03</td>\n",
       "      <td>Reports from the twelve Federal Reserve Distri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-06-14</td>\n",
       "      <td>Reports from the Federal Reserve Districts ind...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-08-09</td>\n",
       "      <td>The information collected for these reports su...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date                                     Statement Text\n",
       "0 2000-01-19  Reports from most Federal Reserve Districts in...\n",
       "1 2000-03-08  Reports from the twelve Federal Reserve Distri...\n",
       "2 2000-05-03  Reports from the twelve Federal Reserve Distri...\n",
       "3 2000-06-14  Reports from the Federal Reserve Districts ind...\n",
       "4 2000-08-09  The information collected for these reports su..."
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Function to extract the statement links for a given year based on its period\n",
    "def extract_yearly_links(year):\n",
    "    if 2000 <= year <= 2010:\n",
    "        url = f\"https://www.federalreserve.gov/monetarypolicy/fomchistorical{year}.htm\"\n",
    "        link_pattern = r'/fomc/beigebook/\\d{4}/\\d{8}/default.htm'\n",
    "    elif 2011 <= year <= 2016:\n",
    "        url = f\"https://www.federalreserve.gov/monetarypolicy/beigebook/beigebook{year}.htm\"\n",
    "        link_pattern = r'/monetarypolicy/beigebook/beigebook\\d{6}.htm'\n",
    "    else:  # 2017-2018\n",
    "        url = f\"https://www.federalreserve.gov/monetarypolicy/beigebook{year}.htm\"\n",
    "        link_pattern = r'/monetarypolicy/beigebook\\d{6}.htm'\n",
    "\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    links = soup.find_all('a', href=True)\n",
    "    \n",
    "    # Filter only the relevant Beige Book links based on the year pattern\n",
    "    relevant_links = [link['href'] for link in links if re.search(link_pattern, link['href'])]\n",
    "    \n",
    "    return relevant_links\n",
    "\n",
    "# Function to extract the statement text from an HTML link\n",
    "def extract_statement_text(link):\n",
    "    base_url = \"https://www.federalreserve.gov\"\n",
    "    full_url = base_url + link if link.startswith('/') else base_url + '/' + link\n",
    "    response = requests.get(full_url)\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "    text = soup.get_text().strip()\n",
    "    \n",
    "    # Phrase that marks the start of relevant content\n",
    "    start_marker = \"the views of Federal Reserve officials.\"\n",
    "    # Find the starting point in the text\n",
    "    start_index = text.find(start_marker)\n",
    "    \n",
    "    if start_index != -1:\n",
    "        # Extract the relevant part after the start_marker\n",
    "        relevant_text = text[start_index + len(start_marker):].strip()\n",
    "        return relevant_text\n",
    "    else:\n",
    "        # If the start marker is not found, return None\n",
    "        return None\n",
    "\n",
    "# Function to extract FOMC statements for a given year range\n",
    "def extract_fomc_statements(start_year, end_year):\n",
    "    all_statements = []\n",
    "    all_dates = []\n",
    "\n",
    "    for year in range(start_year, end_year + 1):\n",
    "        print(f\"Processing year: {year}\")\n",
    "        links = extract_yearly_links(year)\n",
    "\n",
    "        for link in links:\n",
    "            if year <= 2010:\n",
    "                match = re.search(r'/fomc/beigebook/(\\d{4})/(\\d{8})/default.htm', link)\n",
    "            else:\n",
    "                match = re.search(r'beigebook(\\d{4})(\\d{2})', link)\n",
    "\n",
    "            if match:\n",
    "                if year <= 2010:\n",
    "                    date = match.group(2)\n",
    "                else:\n",
    "                    year_str = match.group(1)\n",
    "                    month = match.group(2)\n",
    "                    date = f\"{year_str}{month}01\"\n",
    "\n",
    "                if date not in all_dates:  # Prevent duplicate entries\n",
    "                    statement_text = extract_statement_text(link)\n",
    "                    if statement_text:  # Only append if the relevant text is found\n",
    "                        all_dates.append(date)\n",
    "                        all_statements.append(statement_text)\n",
    "\n",
    "    # Convert the date strings to datetime objects for filtering and sorting\n",
    "    all_dates = pd.to_datetime(all_dates, format='%Y%m%d')\n",
    "    \n",
    "    # Create a DataFrame with the filtered and ordered data\n",
    "    df = pd.DataFrame({'Date': all_dates, 'Statement Text': all_statements})\n",
    "\n",
    "    # Filter out the specific dates we want to remove\n",
    "    df = df[~df['Date'].isin(pd.to_datetime(dates_to_remove))]\n",
    "\n",
    "    df = df.sort_values(by='Date')  # Sort by date\n",
    "\n",
    "    return df\n",
    "\n",
    "# Extract statements for each period and combine them\n",
    "df_2000_2010 = extract_fomc_statements(2000, 2010)\n",
    "df_2011_2016 = extract_fomc_statements(2011, 2016)\n",
    "df_2017_2018 = extract_fomc_statements(2017, 2018)\n",
    "\n",
    "# Merge all the DataFrames into one\n",
    "df_2000_2018 = pd.concat([df_2000_2010, df_2011_2016, df_2017_2018])\n",
    "\n",
    "# Save the final DataFrame to a single Excel file\n",
    "output_file_path = 'Data/FOMC_statements_2000_2018_filtered_cleaned.xlsx'\n",
    "df_2000_2018.to_excel(output_file_path, index=False)\n",
    "\n",
    "# Display the merged DataFrame for verification\n",
    "df_2000_2018.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Statement Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2024-03-20</td>\n",
       "      <td>March 20, 2024\\nFederal Reserve issues FOMC st...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2024-05-01</td>\n",
       "      <td>May 01, 2024\\nFederal Reserve issues FOMC stat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2024-06-12</td>\n",
       "      <td>June 12, 2024\\nFederal Reserve issues FOMC sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2024-07-31</td>\n",
       "      <td>July 31, 2024\\nFederal Reserve issues FOMC sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2024-09-18</td>\n",
       "      <td>September 18, 2024\\nFederal Reserve issues FOM...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date                                     Statement Text\n",
       "1  2024-03-20  March 20, 2024\\nFederal Reserve issues FOMC st...\n",
       "2  2024-05-01  May 01, 2024\\nFederal Reserve issues FOMC stat...\n",
       "3  2024-06-12  June 12, 2024\\nFederal Reserve issues FOMC sta...\n",
       "4  2024-07-31  July 31, 2024\\nFederal Reserve issues FOMC sta...\n",
       "5  2024-09-18  September 18, 2024\\nFederal Reserve issues FOM..."
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge all the DataFrames into one\n",
    "df_all_years = pd.concat([df_2000_2018, df_2019_2024])\n",
    "\n",
    "# Ensure 'Date' column is in datetime format\n",
    "df_all_years['Date'] = pd.to_datetime(df_all_years['Date']).dt.strftime('%Y-%m-%d')\n",
    "\n",
    "# Order by date\n",
    "df_all_years = df_all_years.sort_values(by='Date')\n",
    "\n",
    "#df_all_years.head()\n",
    "df_all_years.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count      170.000000\n",
      "mean      7124.458824\n",
      "std       7892.273684\n",
      "min        163.000000\n",
      "25%        526.500000\n",
      "50%       2032.000000\n",
      "75%      17232.500000\n",
      "max      20050.000000\n",
      "Name: Word Count, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Add the word count for each statement\n",
    "df_all_years['Word Count'] = df_all_years['Statement Text'].apply(lambda x: len(x.split()))\n",
    "\n",
    "# Calculate summary statistics for the word count\n",
    "summary_stats = df_all_years['Word Count'].describe()\n",
    "\n",
    "# Display the summary statistics\n",
    "print(summary_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final DataFrame and summary statistics to a single Excel file\n",
    "output_file_path = 'Output_Files/FOMC_statements_with_summary_statistics.xlsx'\n",
    "\n",
    "# Save both the data and the summary statistics in the same Excel file using ExcelWriter\n",
    "with pd.ExcelWriter(output_file_path) as writer:\n",
    "    df_all_years.to_excel(writer, sheet_name='FOMC Statements', index=False)\n",
    "    summary_stats.to_frame(name='Summary Statistics').to_excel(writer, sheet_name='Summary Statistics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. (30 points) Follow Tadle (2022) section 3.1 (pages 4-8) to measure the tone of each FOMC statement. Here is a brief description of his method and some programming suggestions.\n",
    "\n",
    "- Split the statement into sentences using sent tokenize from the nltk package in Python.\n",
    "\n",
    "- Remove punctuation and capitalization.\n",
    "\n",
    "- Remove all sentences that do not contain any keywords defined as those from the hawkish or dovish lists in the Excel file provided (Tadle 2022 keywords).\n",
    "\n",
    "- For each remaining sentence, count the number of positive and negative keywords, adjusting for negation terms. For example, “When a positive term is in the proximity (occurred after three words or less) of a negation term, then its effect is counted as negative.” See Table 3 in Tadle (2022) for a list of examples.\n",
    "\n",
    "- Use equation (1) on page 7 of Tadle (2022) to assign sentiment scores to each sentence. Footnote 13 on page 7 tells you how to handle sentences that contain both hawkish and dovish keywords.\n",
    "\n",
    "- Aggregate across sentences to get the document-level score for each FOMC statement by following equation (2) on page 7 of Tadle (2022).\n",
    "\n",
    "Plot the computed sentiment of each FOMC statement since January 2000. Please plot the dots only; donot connect dots with lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>hawkish</th>\n",
       "      <th>dovish</th>\n",
       "      <th>positive</th>\n",
       "      <th>negative</th>\n",
       "      <th>negation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>accomodation</td>\n",
       "      <td>abating</td>\n",
       "      <td>adverse</td>\n",
       "      <td>fail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>economy</td>\n",
       "      <td>downturn</td>\n",
       "      <td>augmented</td>\n",
       "      <td>contracting</td>\n",
       "      <td>less</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>equity</td>\n",
       "      <td>unemployment</td>\n",
       "      <td>booming</td>\n",
       "      <td>damping</td>\n",
       "      <td>never</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>housing</td>\n",
       "      <td>devastation</td>\n",
       "      <td>elevating</td>\n",
       "      <td>declining</td>\n",
       "      <td>no</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>inflationary</td>\n",
       "      <td>recession</td>\n",
       "      <td>extended</td>\n",
       "      <td>depressed</td>\n",
       "      <td>not</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        hawkish        dovish   positive     negative negation\n",
       "0      business  accomodation    abating      adverse     fail\n",
       "1       economy      downturn  augmented  contracting     less\n",
       "2        equity  unemployment    booming      damping    never\n",
       "3       housing   devastation  elevating    declining       no\n",
       "4  inflationary     recession   extended    depressed      not"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the keywords into a Pandas DataFrame\n",
    "keywords_df = pd.read_excel('Data/Tadle_2022_keywords.xlsx')\n",
    "\n",
    "# Display the first 5 rows of the dataframe\n",
    "keywords_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting DataFrame into a Dictionary\n",
    "# Keys: Word Categories ('hawkish', 'dovish', 'positive', 'negative', 'negation')\n",
    "# Values: List of words associated with each word category\n",
    "\n",
    "list_of_categories = ['hawkish', 'dovish', 'positive', 'negative', 'negation']\n",
    "\n",
    "# Empty Dictionary to store word categories and associated values\n",
    "keywords_dict = {}\n",
    "\n",
    "# Looping over the dataframe to populate the dictionary\n",
    "for cat_ in list_of_categories:\n",
    "    cat_words_list = keywords_df[cat_].dropna().tolist()\n",
    "    if cat_ not in keywords_dict:\n",
    "        keywords_dict[cat_] = cat_words_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function designed to do the following\n",
    "# 1. Split the statement into sentences\n",
    "# 2. Perform data cleaning. i.e. Remove punctuations and Capitalization\n",
    "# 3. Extract relevant sentences, where relevant is defined as sentence that contains\n",
    "# atleast 1 Hawkish or Dovish keyword\n",
    "# 4. Return a dictionary with original statement divided into sentences against each date and Return a dictionary\n",
    "# with Relevant sentences against each date\n",
    "\n",
    "def clean_process_sentences(statements_df, hawkish_word_list, dovish_word_list):\n",
    "\n",
    "    \"\"\"\n",
    "    Input:\n",
    "        statments_df (DataFrame): A DataFrame of FOMC statements\n",
    "        hawkish_word_list (list): List of hawkish keywords.\n",
    "        dovish_word_list (list): List of dovish keywords.\n",
    "\n",
    "    Output:\n",
    "        tokenized_sentence_dict (dictionary): A dictionary where key-value pairs are\n",
    "                                              key: statement date\n",
    "                                              values: list of tokenized sentences for each key\n",
    "        hawkish_dovish_sentence_dict (dictionary): A dictionary where key-value pairs are\n",
    "                                                   key: statement date\n",
    "                                                   values: list of Relevant sentences for each key  \n",
    "    \"\"\"\n",
    "\n",
    "    statements_df = statements_df.copy()\n",
    "    #statements_df['Date'] = statements_df['Date'].strftime('%Y-%m-%d')\n",
    "    \n",
    "    keyword_list_combined = hawkish_word_list.copy() + dovish_word_list.copy()\n",
    "    \n",
    "    tokenized_sentence_dict = {}\n",
    "    clean_sentences_dict = {}\n",
    "    hawkish_dovish_sentence_dict = {}\n",
    "    irrelevant_sentences_dict = {}\n",
    "\n",
    "    for index, stmnt in statements_df.iterrows():\n",
    "        \n",
    "        tokenized_sentences = \"\"\n",
    "        statement = str(stmnt['Statement Text'])\n",
    "        tokenized_sentences = sent_tokenize(statement)\n",
    "        \n",
    "        if stmnt['Date'] not in tokenized_sentence_dict:\n",
    "            tokenized_sentence_dict[stmnt['Date']] = tokenized_sentences\n",
    "        \n",
    "        clean_sentence = \"\"\n",
    "        hawkish_dovish_sentence = \"\"\n",
    "        irrelevant_sentence = \"\"\n",
    "        \n",
    "        clean_sentences_list = []\n",
    "        hawkish_dovish_sentence_list = []\n",
    "        irrelevant_sentences_list = []\n",
    "        \n",
    "        for sntnc in tokenized_sentences:    \n",
    "        \n",
    "            tokenized_words = \"\"\n",
    "            tokenized_words = word_tokenize(sntnc)\n",
    "            clean_tokenized_words = [word_.lower() for word_ in tokenized_words if word_ not in string.punctuation]\n",
    "            clean_sentence = ' '.join(clean_tokenized_words)\n",
    "            clean_sentences_list.append(clean_sentence)\n",
    "            \n",
    "            if any(word_ in keyword_list_combined for word_ in clean_tokenized_words):\n",
    "                hawkish_dovish_sentence = clean_sentence\n",
    "                hawkish_dovish_sentence_list.append(hawkish_dovish_sentence)\n",
    "            else:\n",
    "                irrelevant_sentence = clean_sentence\n",
    "                irrelevant_sentences_list.append(irrelevant_sentence)\n",
    "            \n",
    "        if stmnt['Date'] not in clean_sentences_dict:\n",
    "            clean_sentences_dict[stmnt['Date']] = clean_sentences_list\n",
    "        \n",
    "        if stmnt['Date'] not in hawkish_dovish_sentence_dict:\n",
    "            hawkish_dovish_sentence_dict[stmnt['Date']] = hawkish_dovish_sentence_list\n",
    "        \n",
    "        if stmnt['Date'] not in irrelevant_sentences_dict:\n",
    "            irrelevant_sentences_dict[stmnt['Date']] = irrelevant_sentences_list\n",
    "        \n",
    "    return tokenized_sentence_dict, hawkish_dovish_sentence_dict, irrelevant_sentences_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentence_score(list_of_words, hawkish_word_list, dovish_word_list, \n",
    "                             positive_word_list, negative_word_list,negation_word_list):\n",
    "    \n",
    "    hawkish_word_cnt = 0\n",
    "    dovish_word_cnt = 0\n",
    "    positive_word_cnt = 0\n",
    "    negative_word_cnt = 0\n",
    "    negation_proximity_threshold = 3\n",
    "\n",
    "    for idx, word_ in enumerate(list_of_words):\n",
    "        if word_ in hawkish_word_list:\n",
    "            #print(word_)\n",
    "            hawkish_word_cnt += 1\n",
    "        \n",
    "        if word_ in dovish_word_list:\n",
    "            dovish_word_cnt += 1\n",
    "        \n",
    "        # Positive Word Count logic\n",
    "        # First check for negation criterion, if it is True then increment Negative Word Count by 1\n",
    "        if word_ in positive_word_list:\n",
    "            left_limit = max(0, idx - negation_proximity_threshold)\n",
    "            if any(negation_word in list_of_words[left_limit : idx] for negation_word in negation_word_list):\n",
    "                negative_word_cnt += 1\n",
    "        # Otherwise, increase Positive Word Count by 1\n",
    "            else:\n",
    "                positive_word_cnt += 1\n",
    "        \n",
    "        # Negative Word Count logic\n",
    "        # First check for negation criterion, if it is True then increment Positive Word Count by 1\n",
    "        elif word_ in negative_word_list:\n",
    "            left_limit = max(0, idx - 1)\n",
    "            if any(negation_word in list_of_words[left_limit : idx] for negation_word in negation_word_list):\n",
    "                positive_word_cnt += 1\n",
    "        # Otherwise, increase Negative Word Count by 1\n",
    "            else:\n",
    "                negative_word_cnt += 1\n",
    "    #print(list_of_words)\n",
    "    #print(f\"Hawkish: {hawkish_word_cnt}, Dovish: {dovish_word_cnt}, Positive: {positive_word_cnt}, Negative: {negative_word_cnt}\")\n",
    "    \n",
    "    if hawkish_word_cnt > 0:\n",
    "        #print('Enter Hawkish Loop')\n",
    "        if positive_word_cnt == negative_word_cnt:\n",
    "            #print('pos = negative')\n",
    "            return 1  # Only hawkish words\n",
    "        elif positive_word_cnt > negative_word_cnt:\n",
    "            #print('pos > neg')\n",
    "            return 1  # Positive hawkish sentiment\n",
    "        elif negative_word_cnt > positive_word_cnt:\n",
    "            #print('neg > pos')\n",
    "            return -1  # Negative hawkish sentiment\n",
    "\n",
    "    elif dovish_word_cnt > 0:\n",
    "        #print('Enter Dovish Loop')\n",
    "        if positive_word_cnt == negative_word_cnt:\n",
    "            #print('pos = negative')\n",
    "            return -1  # Only dovish words\n",
    "        elif positive_word_cnt > negative_word_cnt:\n",
    "            #print('pos > neg')\n",
    "            return -1  # Negative dovish sentiment\n",
    "        elif negative_word_cnt > positive_word_cnt:\n",
    "            #print('neg > pos')\n",
    "            return 1  # Positive dovish sentiment\n",
    "    else:\n",
    "        #print('Neutral')\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function assigns score to each sentence\n",
    "# Utilizes the function calculate_sentence_score, which returns 1, -1, 0 based on the logic\n",
    "\n",
    "def assign_sentence_score(relevant_sentence_dict,hawkish_word_list, dovish_word_list,\n",
    "                          positive_word_list, negative_word_list, negation_word_list):\n",
    "    \n",
    "    date_sentence_score_dict = {}\n",
    "    \n",
    "    for date, list_of_sentences in relevant_sentence_dict.items():\n",
    "        \n",
    "        sentence_score_dict = {}\n",
    "\n",
    "        for sntnc in list_of_sentences:\n",
    "            \n",
    "            sentence_score = 0\n",
    "            tokenized_words_list = word_tokenize(sntnc)\n",
    "            sentence_score = calculate_sentence_score(tokenized_words_list, hawkish_word_list, dovish_word_list, \n",
    "                                                      positive_word_list, negative_word_list,negation_word_list)\n",
    "            \n",
    "            sentence_score_dict[sntnc] = sentence_score\n",
    "        \n",
    "        date_sentence_score_dict[date] = sentence_score_dict\n",
    "    \n",
    "    return date_sentence_score_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assign_document_score(relevant_sentence_dict,hawkish_word_list, dovish_word_list,\n",
    "                          positive_word_list, negative_word_list, negation_word_list):\n",
    "    \n",
    "    date_sentence_score_dict = assign_sentence_score(relevant_sentence_dict, keywords_hawkish, keywords_dovish, \n",
    "                                                     keywords_positive, keywords_negative, keywords_negation)\n",
    "    \n",
    "    date_document_score_index_dict = {}\n",
    "\n",
    "    for date, sentence_score_dict in date_sentence_score_dict.items():\n",
    "        \n",
    "        total_document_score = 0\n",
    "        num_rlvnt_sntncs = 0\n",
    "        \n",
    "        num_rlvnt_sntncs = len(sentence_score_dict.keys())\n",
    "        total_document_score = sum(sentence_score_dict.values())\n",
    "        #print(f\"nd: {num_rlvnt_sntncs}, Total Document Score:{total_document_score}\")\n",
    "        \n",
    "        index_ = 100 * (1/num_rlvnt_sntncs) * total_document_score\n",
    "        date_document_score_index_dict[date] = index_\n",
    "    \n",
    "    return date_document_score_index_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_hawkish = keywords_dict['hawkish']\n",
    "keywords_dovish = keywords_dict['dovish']\n",
    "keywords_positive = keywords_dict['positive']\n",
    "keywords_negative = keywords_dict['negative']\n",
    "keywords_negation = keywords_dict['negation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "sntnc_dict, hwksh_dvsh_sntnc_dict, irlvnt_sntc_dict = clean_process_sentences(df_all_years, keywords_hawkish, keywords_dovish)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keys_to_include = ['2022-01-26']\n",
    "#subset_dict = {key: hwk_dv_dict[key] for key in keys_to_include if key in hwk_dv_dict}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_indx_score_dict = assign_document_score(hwksh_dvsh_sntnc_dict,keywords_hawkish, keywords_dovish, keywords_positive, keywords_negative, keywords_negation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Index Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-19</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-03-08</td>\n",
       "      <td>86.046512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-05-03</td>\n",
       "      <td>89.189189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-06-14</td>\n",
       "      <td>56.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-08-09</td>\n",
       "      <td>44.680851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Date  Index Score\n",
       "0  2000-01-19    75.000000\n",
       "1  2000-03-08    86.046512\n",
       "2  2000-05-03    89.189189\n",
       "3  2000-06-14    56.000000\n",
       "4  2000-08-09    44.680851"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_indx_score_df = pd.DataFrame.from_dict(date_indx_score_dict, orient = 'index')\n",
    "date_indx_score_df.reset_index(inplace = True)\n",
    "date_indx_score_df.rename(columns = {'index':'Date', 0:'Index Score'}, inplace = True)\n",
    "date_indx_score_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame for Index (Sentiment) Scores against each Press Release Date into an excel sheet\n",
    "output_file_path = 'Output_Files/FOMC_statement_date_sentiment_score.xlsx'\n",
    "\n",
    "with pd.ExcelWriter(output_file_path) as writer:\n",
    "    date_indx_score_df.to_excel(writer, sheet_name = 'Sentiment Score', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Index Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2000-01-19</td>\n",
       "      <td>75.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2000-03-08</td>\n",
       "      <td>86.046512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2000-05-03</td>\n",
       "      <td>89.189189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2000-06-14</td>\n",
       "      <td>56.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2000-08-09</td>\n",
       "      <td>44.680851</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date  Index Score\n",
       "0 2000-01-19    75.000000\n",
       "1 2000-03-08    86.046512\n",
       "2 2000-05-03    89.189189\n",
       "3 2000-06-14    56.000000\n",
       "4 2000-08-09    44.680851"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_indx_score_df = date_indx_score_df.reset_index().rename(columns={\"index\": \"Date\"})\n",
    "date_indx_score_df['Date'] = pd.to_datetime(date_indx_score_df['Date'])\n",
    "date_indx_score_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "x and y must be the same size",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[63], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# Scatter plot with dot size based on \"Index Score\"\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[43max\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscatter\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_indx_score_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mDate\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# X-axis: Date\u001b[39;49;00m\n\u001b[1;32m      9\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdate_indx_score_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mIndex Score\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m         \u001b[49m\u001b[38;5;66;43;03m# Y-axis: Index Score\u001b[39;49;00m\n\u001b[1;32m     10\u001b[0m \u001b[43m    \u001b[49m\u001b[43ms\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m40\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                                      \u001b[49m\u001b[38;5;66;43;03m# Set a fixed dot size, or you can replace it with a variable if needed\u001b[39;49;00m\n\u001b[1;32m     11\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolor\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpurple\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m                            \u001b[49m\u001b[38;5;66;43;03m# Color of dots\u001b[39;49;00m\n\u001b[1;32m     12\u001b[0m \u001b[43m    \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.6\u001b[39;49m\u001b[43m                                  \u001b[49m\u001b[38;5;66;43;03m# Transparency for better visualization\u001b[39;49;00m\n\u001b[1;32m     13\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# Set title and labels\u001b[39;00m\n\u001b[1;32m     16\u001b[0m ax\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSentiment Score of Each FOMC Statement Since January 2000\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tsaf/lib/python3.12/site-packages/matplotlib/__init__.py:1465\u001b[0m, in \u001b[0;36m_preprocess_data.<locals>.inner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1462\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m   1463\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minner\u001b[39m(ax, \u001b[38;5;241m*\u001b[39margs, data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m   1464\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m data \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1465\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msanitize_sequence\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1467\u001b[0m     bound \u001b[38;5;241m=\u001b[39m new_sig\u001b[38;5;241m.\u001b[39mbind(ax, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1468\u001b[0m     auto_label \u001b[38;5;241m=\u001b[39m (bound\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mget(label_namer)\n\u001b[1;32m   1469\u001b[0m                   \u001b[38;5;129;01mor\u001b[39;00m bound\u001b[38;5;241m.\u001b[39mkwargs\u001b[38;5;241m.\u001b[39mget(label_namer))\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tsaf/lib/python3.12/site-packages/matplotlib/axes/_axes.py:4655\u001b[0m, in \u001b[0;36mAxes.scatter\u001b[0;34m(self, x, y, s, c, marker, cmap, norm, vmin, vmax, alpha, linewidths, edgecolors, plotnonfinite, **kwargs)\u001b[0m\n\u001b[1;32m   4653\u001b[0m y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mma\u001b[38;5;241m.\u001b[39mravel(y)\n\u001b[1;32m   4654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m!=\u001b[39m y\u001b[38;5;241m.\u001b[39msize:\n\u001b[0;32m-> 4655\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx and y must be the same size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   4657\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m s \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4658\u001b[0m     s \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m20\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_internal.classic_mode\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01melse\u001b[39;00m\n\u001b[1;32m   4659\u001b[0m          mpl\u001b[38;5;241m.\u001b[39mrcParams[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlines.markersize\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2.0\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: x and y must be the same size"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Scatter plot with dot size based on \"Index Score\"\n",
    "ax.scatter(\n",
    "    date_indx_score_df[\"Date\"],                # X-axis: Date\n",
    "    date_indx_score_df[\"Index Score\"],         # Y-axis: Index Score\n",
    "    s=40,                                      # Set a fixed dot size, or you can replace it with a variable if needed\n",
    "    color='purple',                            # Color of dots\n",
    "    alpha=0.6                                  # Transparency for better visualization\n",
    ")\n",
    "\n",
    "# Set title and labels\n",
    "ax.set_title(\"Sentiment Score of Each FOMC Statement Since January 2000\")\n",
    "ax.set_xlabel(\"Date\")\n",
    "ax.set_ylabel(\"Sentiment Score\")\n",
    "\n",
    "# Adjust layout and display\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. (10 points) From FRED (Federal Reserve Economic Data, https://fred.stlouisfed.org/) find the following data series: \n",
    "- 1) Federal Funds Target Rate (DFEDTAR) and \n",
    "- 2) Federal Funds Target Range-Upper (DFEDTARU). \n",
    "\n",
    "You can search the “ticker symbol” (Capital letters in parenthesis) for each series given. FOMC used to announce a targeted value for the Federal Funds Rate (FFR) before Dec. 15th, 2008. \n",
    "\n",
    "Since that date, FOMC has announced a range. We will use the upper limit. Compute the change of the target on each of the FOMC statement release date. Plot these changes together with sentiment score you computed in the previous section. What do you find?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "fred_api_key = '6014571ef89b16f87265e5d0b7644011' \n",
    "fred = Fred(api_key = fred_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'xml.etree.ElementTree.Element' object has no attribute 'getchildren'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dfedtar_data \u001b[38;5;241m=\u001b[39m \u001b[43mfred\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_series\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mDFEDTAR\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m dfedtaru_data \u001b[38;5;241m=\u001b[39m fred\u001b[38;5;241m.\u001b[39mget_series(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDFEDTARU\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      4\u001b[0m fed_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame({\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDFEDTAR\u001b[39m\u001b[38;5;124m'\u001b[39m: dfedtar_data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDFEDTARU\u001b[39m\u001b[38;5;124m'\u001b[39m: dfedtaru_data})\n",
      "File \u001b[0;32m/opt/anaconda3/envs/tsaf/lib/python3.12/site-packages/fredapi/fred.py:135\u001b[0m, in \u001b[0;36mFred.get_series\u001b[0;34m(self, series_id, observation_start, observation_end, **kwargs)\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNo data exists for series id: \u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m+\u001b[39m series_id)\n\u001b[1;32m    134\u001b[0m data \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m child \u001b[38;5;129;01min\u001b[39;00m \u001b[43mroot\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetchildren\u001b[49m():\n\u001b[1;32m    136\u001b[0m     val \u001b[38;5;241m=\u001b[39m child\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    137\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m val \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnan_char:\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'xml.etree.ElementTree.Element' object has no attribute 'getchildren'"
     ]
    }
   ],
   "source": [
    "dfedtar_data = fred.get_series('DFEDTAR')\n",
    "dfedtaru_data = fred.get_series('DFEDTARU')\n",
    "\n",
    "fed_data = pd.DataFrame({'DFEDTAR': dfedtar_data, 'DFEDTARU': dfedtaru_data})\n",
    "\n",
    "cutoff_date = pd.Timestamp('2008-12-15')\n",
    "fed_data['Target_Rate'] = fed_data.apply(\n",
    "    lambda row: row['DFEDTAR'] if row.name <= cutoff_date else row['DFEDTARU'], axis=1)\n",
    "\n",
    "fed_data = fed_data.drop(columns=['DFEDTAR', 'DFEDTARU'])\n",
    "\n",
    "fed_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fed_data.index = pd.to_datetime(fed_data.index)\n",
    "date_indx_score_df['Date'] = pd.to_datetime(date_indx_score_df['Date'])\n",
    "\n",
    "merged_data = pd.merge(date_indx_score_df, fed_data, left_on='Date', right_index=True, how='inner')\n",
    "merged_data['Change_Target_Rate'] = merged_data['Target_Rate'].diff()\n",
    "\n",
    "merged_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 6))\n",
    "plt.scatter(merged_data['Change_Target_Rate'], merged_data['Index Score'], alpha=0.7, edgecolor='k')\n",
    "\n",
    "plt.xlabel('Change in Target Rate')\n",
    "plt.ylabel('Index Score')\n",
    "plt.title('Scatter Plot of Change in Target Rate vs. Index Score')\n",
    "\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Measuring the impact on the bond market (40 points)\n",
    "\n",
    "We are going to treat the changes in the FFR targets and the calculated sentiments of FOMC statements a la Tadle (2022) as if they are surprises. See Aruoba and Drechsel (2022) to see why this is not a realistic assumption."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. (5 points) From FRED find the following data series: 1) EFFR, 2) DGS1MO, and 3) DGS10. Make sure that you read their description. Compute the changes of values in these series on the date of FOMC meeting from the previous day. Provide summary statistics including, number of observations, mean, median, and standard deviation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Sets fron FRED:\n",
    "\n",
    "effr_file = 'Data/EFFR.xlsx' #effective federal funds rate\n",
    "dgs1mo_file = 'Data/DGS1MO.xlsx'#1-month treasury constant maturity rate\n",
    "dgs10_file = 'Data/DGS10.xlsx' #10-year treasury constant maturity rate\n",
    "\n",
    "# Standarized the data set by removing the first 10 rows. \n",
    "effr_df = pd.read_excel(effr_file, skiprows=10) #earliest: 2000-07-03 and 2024-10-24\n",
    "dgs1mo_df = pd.read_excel(dgs1mo_file, skiprows=10) #earliest: 2001-07-31 and 2024-10-24\n",
    "dgs10_df = pd.read_excel(dgs10_file, skiprows=10) #earliest: 2000-01-03 and 2024-10-24\n",
    "\n",
    "#From previous\n",
    "Dates_FOMC_announcements = df_all_years['Date']\n",
    "\n",
    "# Merging data\n",
    "df = effr_df.merge(dgs1mo_df, on = 'observation_date', how = 'inner')\n",
    "df = df.merge(dgs10_df, on = 'observation_date', how = 'inner')\n",
    "\n",
    "df.columns = ['Date', 'EFFR', 'DGS1MO', 'DGS10']\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(by='Date').reset_index(drop=True)\n",
    "\n",
    "# Calculate daily changes#differences from 1 day to another. \n",
    "df['EFFR_change'] = df['EFFR'].diff()\n",
    "df['DGS1MO_change'] = df['DGS1MO'].diff()\n",
    "df['DGS10_change'] = df['DGS10'].diff()\n",
    "\n",
    "\n",
    "print(f\"Number of FOMC announcement dates: {len(Dates_FOMC_announcements)}\") #Number of FOMC announcement dates: 170\n",
    "\n",
    "# Mathcing with the exact dates of FOMC announcements\n",
    "fomc_changes = df[df['Date'].isin(Dates_FOMC_announcements)]\n",
    "\n",
    "fomc_changes = fomc_changes[['Date', 'EFFR_change', 'DGS1MO_change', 'DGS10_change']]\n",
    "\n",
    "fomc_changes.head()\n",
    "#fomc_changes.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. (20 points) Let $y_{it}$ denote the change of value in series $i$ in date $t$. Let $MPOL_t$ denote the change of FFR target and $HAWK_t$ sentiment of FOMC statement in date $t$. Run the following three specifications of regression:\n",
    "\n",
    "$\\text{Specification 1: }$ $y_{it} = \\alpha_i + \\beta^p_i \\text{MPOL}_t + \\epsilon_{it}$\n",
    "\n",
    "\n",
    "$\\text{Specification 2: }$ $y_{it} = \\alpha_i + \\beta^s_i \\text{HAWK}_t + \\epsilon_{it}$\n",
    "\n",
    "\n",
    "$\\text{Specification 3: }$ $y_{it} = \\alpha_i + \\beta^p_i \\text{MPOL}_t + \\beta^s_i \\text{HAWK}_t + \\epsilon_{it}$\n",
    "\n",
    "\n",
    "Tabulate the estimated value, t-stats, and goodness-of-fit for 9 regressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_results(dataframe_, y_var, X_vars, intercept_ = True):\n",
    "    \"\"\"\n",
    "    Run a regression model based on user-specified y and X variables.\n",
    "\n",
    "    Parameters:\n",
    "    - dataframe_: pd.DataFrame, containing the data.\n",
    "    - y_var: str, name of the dependent variable (y).\n",
    "    - X_vars: list of str, names of the independent variables (X).\n",
    "    - intercept_: bool, whether to include an intercept term (default is True).\n",
    "\n",
    "    Returns:\n",
    "    - results_df: pd.DataFrame, containing coefficients, t-stats, and R-squared.\n",
    "    \"\"\"\n",
    "    # Extract the dependent and independent variables\n",
    "    y = dataframe_[y_var]\n",
    "    X = dataframe_[X_vars]\n",
    "\n",
    "    if intercept_:\n",
    "        X = sm.add_constant(X)\n",
    "\n",
    "    # Fit the model\n",
    "    model = sm.OLS(y, X).fit()\n",
    "\n",
    "    # Create a results dictionary to store output\n",
    "    results_dict = {\"Coefficient\": model.params, \"t-Statistic\": model.tvalues,\n",
    "                    \"p-Value\": model.pvalues, \"R-Square\": model.rsquared}\n",
    "    \n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df = test\n",
    "list_of_X_vars = [['MOPL'], ['HAWK'], ['MOPL','HAWK']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_var = ['dgs1mo']\n",
    "regression_result_dgs1mo_dict = {}\n",
    "\n",
    "for x_ in list_of_X_vars:\n",
    "    result = regression_results(data_df, y_var, x_)\n",
    "    regression_result_dict[f\"Results against {x_}\"] = result\n",
    "\n",
    "regression_result_dgs1mo_df = pd.DataFrame.from_dict(regression_result_dict, orient = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_var = ['dgs10']\n",
    "\n",
    "regression_result_dgs10_dict = {}\n",
    "\n",
    "for x_ in list_of_X_vars:\n",
    "    result = regression_results(data_df, y_var, x_)\n",
    "    regression_result_dgs10_dict[f\"Results against {x_}\"] = result\n",
    "\n",
    "regression_result_dgs10_df = pd.DataFrame.from_dict(regression_result_dgs10_dict, orient = 'index')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tsaf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
